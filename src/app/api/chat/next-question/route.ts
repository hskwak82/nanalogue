import { NextResponse } from 'next/server'
import { GoogleGenerativeAI } from '@google/generative-ai'
import type { ConversationMessage } from '@/types/database'

function getGeminiClient() {
  return new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '')
}

export async function POST(request: Request) {
  try {
    const { messages, questionCount } = await request.json()

    // First greeting
    if (questionCount === 0) {
      return NextResponse.json({
        question: 'ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ í•˜ë£¨ëŠ” ì–´ë– ì…¨ë‚˜ìš”? í¸í•˜ê²Œ ì´ì•¼ê¸°í•´ ì£¼ì„¸ìš” ğŸ˜Š',
        purpose: 'greeting',
        shouldEnd: false,
      })
    }

    // Closing message after enough conversation
    if (questionCount >= 7) {
      return NextResponse.json({
        question: 'ì˜¤ëŠ˜ ì´ì•¼ê¸° ë‚˜ëˆ ì£¼ì…”ì„œ ê°ì‚¬í•´ìš”. ì´ì œ ë§ì”€í•´ì£¼ì‹  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì˜¤ëŠ˜ì˜ ì¼ê¸°ë¥¼ ì‘ì„±í•´ ë“œë¦´ê²Œìš”.',
        purpose: 'closing',
        shouldEnd: true,
      })
    }

    // Generate natural conversational response
    const conversationContext = messages
      .map(
        (m: ConversationMessage) =>
          `${m.role === 'user' ? 'ì‚¬ìš©ì' : 'AI'}: ${m.content}`
      )
      .join('\n')

    const genAI = getGeminiClient()
    const model = genAI.getGenerativeModel({ model: 'gemini-2.5-flash' })

    // Determine conversation phase
    let phaseGuidance = ''
    if (questionCount <= 2) {
      phaseGuidance = 'ì´ˆë°˜ ëŒ€í™”: ì‚¬ìš©ìì˜ í•˜ë£¨ ì „ë°˜ì ì¸ ê¸°ë¶„ê³¼ ì£¼ìš” ì¼ê³¼ë¥¼ íŒŒì•…í•˜ì„¸ìš”.'
    } else if (questionCount <= 4) {
      phaseGuidance = 'ì¤‘ë°˜ ëŒ€í™”: ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ë‚´ìš© ì¤‘ í¥ë¯¸ë¡œìš´ ë¶€ë¶„ì„ ë” ê¹Šì´ íƒìƒ‰í•˜ì„¸ìš”. ê°ì •ì´ë‚˜ êµ¬ì²´ì ì¸ ìƒí™©ì„ ë¬¼ì–´ë³´ì„¸ìš”.'
    } else {
      phaseGuidance = 'í›„ë°˜ ëŒ€í™”: ê°ì‚¬í–ˆë˜ ì , ë‚´ì¼ ê³„íš, ë˜ëŠ” ì˜¤ëŠ˜ì˜ êµí›ˆ ë“± í•˜ë£¨ë¥¼ ë§ˆë¬´ë¦¬í•˜ëŠ” ì§ˆë¬¸ì„ í•˜ì„¸ìš”.'
    }

    const prompt = `ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ ì¹œêµ¬ ê°™ì€ AIì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ í•˜ë£¨ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë“¤ìœ¼ë©´ì„œ ì¼ê¸° ì‘ì„±ì„ ìœ„í•œ ì •ë³´ë¥¼ ëª¨ìë‹ˆë‹¤.

ì¤‘ìš” ì§€ì¹¨:
1. ë¨¼ì € ì‚¬ìš©ìê°€ ë°©ê¸ˆ ë§í•œ ë‚´ìš©ì— ëŒ€í•´ ì§§ê²Œ ê³µê°í•˜ê±°ë‚˜ ë°˜ì‘í•´ì£¼ì„¸ìš”
2. ê·¸ ë‹¤ìŒ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ëŠ” ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”
3. ë§ˆì¹˜ ì¹œí•œ ì¹œêµ¬ì™€ ëŒ€í™”í•˜ë“¯ í¸ì•ˆí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”
4. ë„ˆë¬´ í˜•ì‹ì ì´ê±°ë‚˜ ë”±ë”±í•˜ì§€ ì•Šê²Œ, êµ¬ì–´ì²´ë¡œ ë§í•˜ì„¸ìš”
5. ì´ëª¨ì§€ëŠ” ì ì ˆíˆ ì‚¬ìš©í•´ë„ ë©ë‹ˆë‹¤

ì‘ë‹µ í˜•ì‹:
- ê³µê°/ë°˜ì‘ (1-2ë¬¸ì¥) + ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²° ì§ˆë¬¸ (1ë¬¸ì¥)
- ì „ì²´ 2-3ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ ìœ ì§€

${phaseGuidance}

ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”:
${conversationContext}

ë‹¤ìŒ ì‘ë‹µì„ ìƒì„±í•˜ì„¸ìš” (ê³µê° + ì§ˆë¬¸):`

    const result = await model.generateContent(prompt)
    const response = await result.response
    const question =
      response.text() || 'ê·¸ë ‡êµ°ìš”, ë” ìì„¸íˆ ì´ì•¼ê¸°í•´ ì£¼ì‹¤ ìˆ˜ ìˆì–´ìš”?'

    return NextResponse.json({
      question: question.trim(),
      purpose: 'conversation',
      shouldEnd: false,
    })
  } catch (error) {
    console.error('Error generating question:', error)

    // Fallback response
    return NextResponse.json({
      question: 'ê·¸ë ‡êµ°ìš”. ë” ìì„¸íˆ ì´ì•¼ê¸°í•´ ì£¼ì‹¤ ìˆ˜ ìˆì–´ìš”?',
      purpose: 'fallback',
      shouldEnd: false,
    })
  }
}
